{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import transformers\n",
    "if not hasattr(transformers, 'is_torch_npu_available'):\n",
    "    transformers.is_torch_npu_available = lambda: False\n",
    "\n",
    "# Fix transformers.utils\n",
    "import transformers.utils as utils\n",
    "if not hasattr(utils, 'is_torch_npu_available'):\n",
    "    utils.is_torch_npu_available = lambda: False\n",
    "\n",
    "# Fix import_utils\n",
    "import transformers.utils.import_utils as import_utils\n",
    "\n",
    "# Add all potentially missing functions\n",
    "missing_functions = {\n",
    "    'is_torch_npu_available': lambda: False,\n",
    "    'is_torch_mps_available': lambda: False,\n",
    "    'is_torch_xpu_available': lambda: False,\n",
    "    'is_torch_tpu_available': lambda: False,\n",
    "    'is_nltk_available': lambda: False,\n",
    "    'is_torch_neuroncore_available': lambda: False,\n",
    "    'is_torch_fx_available': lambda: False,\n",
    "}\n",
    "\n",
    "for func_name, func in missing_functions.items():\n",
    "    if not hasattr(import_utils, func_name):\n",
    "        setattr(import_utils, func_name, func)\n",
    "    if not hasattr(transformers, func_name):\n",
    "        setattr(transformers, func_name, func)\n",
    "\n",
    "if not hasattr(import_utils, 'NLTK_IMPORT_ERROR'):\n",
    "    import_utils.NLTK_IMPORT_ERROR = \"NLTK not available\"\n",
    "    print(\"✓ Added NLTK_IMPORT_ERROR\")\n",
    "\n",
    "if not hasattr(import_utils, 'is_nltk_available'):\n",
    "    import_utils.is_nltk_available = lambda: False\n",
    "    print(\"✓ Added is_nltk_available\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from sklearn.utils import shuffle\n",
    "import argparse\n",
    "from itertools import chain\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from nltk.corpus import stopwords\n",
    "import copy\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "stop_words = stopwords.words('english')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer \n",
    "import PyPDF2\n",
    "import traceback\n",
    "from typing import Tuple, Union, List, Optional\n",
    "import gc\n",
    "from argparse import Namespace\n",
    "import coreferee\n",
    "nlp.add_pipe(\"coreferee\")\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "print(\"✓ snapshot_download imported successfully\")\n",
    "\n",
    "# Check if PEFT is trying to import it wrong\n",
    "try:\n",
    "    import peft\n",
    "    print(f\"PEFT version: {peft.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"PEFT import error: {e}\")\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"device \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import textwrap\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf(pdf_path):\n",
    "    print(\"pdf_path:\", pdf_path)\n",
    "    text = \"\"\n",
    "    try:\n",
    "        reader = PyPDF2.PdfReader(pdf_path)\n",
    "        num_pages = len(reader.pages)\n",
    "        print(\"Number of pages\",num_pages)\n",
    "        for page_num in range(num_pages):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd() + \"/inputText/request/\"\n",
    "workdir=os.listdir(base_path)\n",
    "if '.DS_Store' in workdir:\n",
    "  workdir.remove('.DS_Store')\n",
    "print (base_path)\n",
    "# List of files in above directory, making sure no directories are included\n",
    "#files = (file for file in os.listdir(text_path) if os.path.isfile(os.path.join(text_path, file)))\n",
    "def get_files(path):\n",
    "    file_names = [file_name for file_name in os.listdir(base_path) if os.path.isfile(os.path.join(base_path, file_name))]\n",
    "    file_paths = [os.path.join(base_path, file_name) for file_name in file_names]\n",
    "    return file_paths\n",
    "\n",
    "file_paths=get_files(base_path)\n",
    "\n",
    "for file_path in file_paths:\n",
    "    if file_path.endswith('.pdf'):\n",
    "        key_name = Path(file_path).stem\n",
    "        filePath= os.path.join(base_path,key_name)\n",
    "        outFile = os.path.join(base_path,f\"{key_name}.txt\")\n",
    "        # outFile = ''.join(base_path,f\"/{key_name}.txt\")\n",
    "        print(f\"Processing {filePath}...\")\n",
    "        with open(file_path, 'rb') as f:\n",
    "            text = extract_pdf(f)\n",
    "            with open(outFile, \"w\", encoding=\"utf-8\") as extract:\n",
    "                extract.write(text)\n",
    "            print(f\"Extracted text from {file_path} and saved to {outFile}\")\n",
    "\n",
    "    elif file_path.endswith('.txt'):\n",
    "        outFile = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def readTextFile(path):\n",
    "#     outText = []\n",
    "#     with open(outFile, \"r\") as file:\n",
    "#         # mypdf =json.load(file)\n",
    "#         T = mypdf.readlines(file)\n",
    "#         final = [i.replace('\\r\\n',' ').replace('b\\r\\n', '').replace('\\n', '')  for i in T]\n",
    "#         outText.append(T)\n",
    "#     sentText= extract_sentences(outText)\n",
    "#     final = [i.replace(\"\\n'\",\" \") for i in sentText]\n",
    "#     #result = re.sub(r'http\\S+', '', str(final))\n",
    "#     newFinal =[re.sub(\"[\\(%\\[].*?[\\)\\n]]\", \" \", i) for i in final]\n",
    "#     newFinal =[re.sub(r'http\\S+', '', i) for i in newFinal]\n",
    "#     df2 =pd.DataFrame(final, columns= ['sentences'])\n",
    "#     df3 =pd.DataFrame(newFinal, columns= ['sentences'])\n",
    "\n",
    "#     for i in range(0, len(df2)):\n",
    "#         df2.at[i, 'sentences_new'] = re.sub(r'\\b(\\d+\\w{1,2})\\b',\" \",str(df2.loc[i,'sentences']))\n",
    "#         df2.at[i, 'sentences_new'] = re.sub(r'[\\d+]',\" \",str(df2.loc[i,'sentences_new']))\n",
    "#         df2.at[i, 'sentences_new'] = re.sub(r'[^\\w\\s]',\" \",str(df2.loc[i,'sentences_new']))\n",
    "#         #df2.at[i, 'sentences_new'] = re.sub(r'\\b\\w{1}\\b',\" \",str(df2.loc[i,'sentences_new']))\n",
    "#         df2['text'] = df2['sentences_new'].str.replace(r\"\\s*https?:'//\\S+(\\s+|$)\", \" \").str.strip()\n",
    "#         df2['text2'] = df2['text'].str.replace(r\"ufeff\", \" \").str.strip()\n",
    "#     return df2\n",
    "\n",
    "def readTextFile(path):   \n",
    "    outText = []\n",
    "    for file_path in path:\n",
    "        if file_path.endswith('.txt'):\n",
    "                try:\n",
    "                    txtFile= open(file_path, \"r\")\n",
    "                    mypdf = txtFile.readlines()\n",
    "                    outText.append(mypdf)\n",
    "                except FileNotFoundError:\n",
    "                    print(\"File not found.\")\n",
    "                    print(traceback.format_exc())\n",
    "                except Exception as e:\n",
    "                    print(\"Error:\", e)\n",
    "                finally:\n",
    "                    if 'txtFile' in locals():\n",
    "                        txtFile.close()\n",
    "\n",
    "    print(\"raw text has been read\")\n",
    "    return outText\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## updated sentence extraction to include co-reference resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_sentences(texts):\n",
    "#         content = nlp(str(texts))\n",
    "#         content_resolved = content._.coref_resolved\n",
    "#         # extract sentences from the resolved content\n",
    "#         doc= nlp(content_resolved)\n",
    "#         sentences=[x.text for x in doc.sents]\n",
    "#         return sentences\n",
    "\n",
    "# # formatting and cleaning text\n",
    "# def format(outText):\n",
    "#         \"\"\"Fix syntax issues in incoming text fields, before any further processing\n",
    "#         \"\"\"\n",
    "#         extracted_sentences= extract_sentences(outText)\n",
    "#         texts= ' '.join(extracted_sentences)\n",
    "#         print(\"sample sentences\", extracted_sentences[:10])\n",
    "#         return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update for the above function\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class SentenceProcessor:\n",
    "    \"\"\"Process text for entity extraction and question generation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp = nlp\n",
    "        self.entities = set()\n",
    "        self.entity_mentions = defaultdict(list)\n",
    "    \n",
    "    def process_text(self, text):\n",
    "        \"\"\"Main processing function that returns resolved text and entities\"\"\"\n",
    "        \n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Step 1: Resolve coreferences\n",
    "        resolved_text = self.get_coreferences(doc)\n",
    "        \n",
    "        # Step 2: Process resolved text to get clean sentences\n",
    "        resolved_doc = self.nlp(resolved_text)\n",
    "        \n",
    "        # Step 3: Extract entities from resolved text\n",
    "        corpus_entities = self.entity_extraction(resolved_doc)\n",
    "        \n",
    "        # Step 4: Get sentences for question generation\n",
    "        sentences = self.get_sentences(resolved_doc)\n",
    "        print(\"sentences type\",type(sentences))\n",
    "        print(\"corpus_entities type\",type(corpus_entities))\n",
    "\n",
    "        return sentences, corpus_entities\n",
    "        # return {\n",
    "        #     'sentences': sentences,\n",
    "        #     'corpus_entities': corpus_entities\n",
    "        # }\n",
    "    \n",
    "    def get_coreferences(self, doc):\n",
    "        \"\"\"Resolve all coreferences in the texts\"\"\"\n",
    "        \n",
    "        chains = doc._.coref_chains\n",
    "        \n",
    "        if not chains:\n",
    "            return doc.text\n",
    "        \n",
    "        # Build token replacement map\n",
    "        token_replacements = {}\n",
    "        \n",
    "        for chain in chains:\n",
    "            # Find the most descriptive mention (usually the first noun phrase)\n",
    "            main_mention = self.find_ent_ref(chain)\n",
    "            main_text = \" \".join([token.text for token in main_mention])\n",
    "            \n",
    "            # Map all other mentions to the main mention\n",
    "            for mention in chain:\n",
    "                if mention != main_mention:\n",
    "                    # Store the span to be replaced\n",
    "                    start_token = mention[0].i\n",
    "                    end_token = mention[-1].i\n",
    "                    token_replacements[(start_token, end_token)] = main_text\n",
    "        \n",
    "        # Reconstruct text with replacements\n",
    "        resolved_tokens = []\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(doc):\n",
    "            replaced = False\n",
    "            \n",
    "            # Check if current position starts a mention to be replaced\n",
    "            for (start, end), replacement in token_replacements.items():\n",
    "                if i == start:\n",
    "                    resolved_tokens.append(replacement)\n",
    "                    # Add spacing if needed\n",
    "                    if end + 1 < len(doc) and not doc[end + 1].is_punct:\n",
    "                        resolved_tokens.append(\" \")\n",
    "                    i = end + 1\n",
    "                    replaced = True\n",
    "                    break\n",
    "            \n",
    "            if not replaced:\n",
    "                # Keep original token with its spacing\n",
    "                resolved_tokens.append(doc[i].text_with_ws)\n",
    "                i += 1\n",
    "        \n",
    "        resolved_text = \"\".join(resolved_tokens)\n",
    "        \n",
    "        # Clean up any double spaces\n",
    "        resolved_text = re.sub(r'\\s+', ' ', resolved_text)\n",
    "        resolved_text = re.sub(r'\\s+([.,!?;:])', r'\\1', resolved_text)\n",
    "        \n",
    "        return resolved_text\n",
    "    \n",
    "    def find_ent_ref(self, chain):\n",
    "        \"\"\"Find the most descriptive mention in a coreference chain\"\"\"\n",
    "        \n",
    "        best_reference = chain[0]\n",
    "        best_score = 0\n",
    "        \n",
    "        for ref in chain:\n",
    "            score = 0\n",
    "            # Prefer non-pronouns\n",
    "            if not all(token.pos_ == \"PRON\" for token in ref):\n",
    "                score += 10\n",
    "            if any(token.pos_ == \"PROPN\" for token in ref) or any(token.ent_type_ for token in ref):\n",
    "                 score += 5\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_reference = ref\n",
    "        \n",
    "        return best_reference\n",
    "    \n",
    "    def entity_extraction(self, doc):\n",
    "        \"\"\"Extract all unique entities from resolved text\"\"\"\n",
    "        \n",
    "        named_entities = {\n",
    "            'PERSON': set(),\n",
    "            'ORG': set(),\n",
    "            'GPE': set(),  \n",
    "            'DATE': set(),\n",
    "            'TIME': set(),\n",
    "            'MONEY': set(),\n",
    "            'PERCENT': set(),\n",
    "            'FACILITY': set(),\n",
    "            'LOC': set(),\n",
    "            'PRODUCT': set(),\n",
    "            'EVENT': set(),\n",
    "            'WORK_OF_ART': set(),\n",
    "            'LAW': set(),\n",
    "            'LANGUAGE': set(),\n",
    "            'QUANTITY': set(),\n",
    "            'ORDINAL': set(),\n",
    "            'CARDINAL': set(),\n",
    "            'MISC': set()  # Miscellaneous entities\n",
    "        }\n",
    "        \n",
    "        # Extract named entities\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in named_entities:\n",
    "                named_entities[ent.label_].add(ent.text)\n",
    "            else:\n",
    "                named_entities['MISC'].add(ent.text)\n",
    "        \n",
    "        # Also extract noun phrases that might be important entities\n",
    "        noun_phrases = set()\n",
    "        for chunk in doc.noun_chunks:\n",
    "            # Filter out pronouns and very common words\n",
    "            if (not all(token.pos_ == \"PRON\" for token in chunk) and \n",
    "                len(chunk.text) > 2 and \n",
    "                chunk.text.lower() not in ['the', 'a', 'an']):\n",
    "                noun_phrases.add(chunk.text)\n",
    "        \n",
    "        named_entities['NOUN_PHRASES'] = noun_phrases\n",
    "        \n",
    "        # Remove empty categories\n",
    "        named_entities = {k: v for k, v in named_entities.items() if v}\n",
    "        \n",
    "        # Create a summary of all unique entities\n",
    "        unique_entities = set()\n",
    "        for entity_list in named_entities.values():\n",
    "            unique_entities.update(entity_list)\n",
    "        \n",
    "        return {\n",
    "            'by_type': named_entities,\n",
    "            'all_unique': sorted(unique_entities)\n",
    "        }\n",
    "    \n",
    "    def get_sentences(self, doc):\n",
    "        \"\"\"Prepare sentences for question generation\n",
    "        use scores to prioritize sentences with useful information.\n",
    "        \"\"\"\n",
    "        \n",
    "        sentences = []\n",
    "        \n",
    "        for sent_idx, sent in enumerate(doc.sents):\n",
    "            # Extract sentence information\n",
    "            sentence_entities = [(ent.text, ent.label_) for ent in sent.ents]\n",
    "            \n",
    "            # Calculate sentence quality score for Quality assessment\n",
    "            assessment_score = 0\n",
    "            \n",
    "            # Prefer sentences with entities\n",
    "            assessment_score += len(sentence_entities) * 2\n",
    "            \n",
    "            # # Prefer sentences with facts (dates, numbers, etc.)\n",
    "            if any(ent[1] in ['DATE', 'TIME', 'MONEY', 'PERCENT', 'QUANTITY'] \n",
    "                   for ent in sentence_entities):\n",
    "                assessment_score += 3\n",
    "            \n",
    "            # Prefer sentences with proper nouns\n",
    "            if any(token.pos_ == \"PROPN\" for token in sent):\n",
    "                assessment_score += 2\n",
    "            \n",
    "            # Prefer sentences of moderate length (not too short, not too long)\n",
    "            word_count = len([token for token in sent if not token.is_punct])\n",
    "            if 5 < word_count < 30:\n",
    "                assessment_score += 2\n",
    "            \n",
    "            sentence_info = {\n",
    "                'text': sent.text,\n",
    "                'index': sent_idx,\n",
    "                'entities': sentence_entities,\n",
    "                'word_count': word_count,\n",
    "                'qa_score': assessment_score,\n",
    "                'best_by_assessment': assessment_score >= 3,\n",
    "                'tokens': [token.text for token in sent]\n",
    "            }\n",
    "            \n",
    "            sentences.append(sentence_info)\n",
    "        \n",
    "        # Sort by Quality score for prioritization\n",
    "        sentences.sort(key=lambda x: x['qa_score'], reverse=True)\n",
    "        print(\"sentence should be a dictionary\",type(sentences))\n",
    "        return sentences\n",
    "    \n",
    "    def get_coref_info(self, doc):\n",
    "        \"\"\"Get coreference chain information\"\"\"\n",
    "        \n",
    "        chains = doc._.coref_chains\n",
    "        if not chains:\n",
    "            return []\n",
    "        \n",
    "        coref_info = []\n",
    "        for chain_idx, chain in enumerate(chains):\n",
    "            mentions = []\n",
    "            for mention in chain:\n",
    "                mention_text = \" \".join([token.text for token in mention])\n",
    "                mention_type = \"pronoun\" if all(token.pos_ == \"PRON\" for token in mention) else \"noun\"\n",
    "                mentions.append({\n",
    "                    'text': mention_text,\n",
    "                    'type': mention_type,\n",
    "                    'start': mention[0].idx,\n",
    "                    'end': mention[-1].idx + len(mention[-1].text)\n",
    "                })\n",
    "            \n",
    "            coref_info.append({\n",
    "                'chain_id': chain_idx,\n",
    "                'mentions': mentions,\n",
    "                'main_entity': mentions[0]['text'] if mentions else None\n",
    "            })\n",
    "        \n",
    "        return coref_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## updated text preprocessor compartible with most text and technical documents after pdf extraction to txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the preprocessor with common patterns.\"\"\"\n",
    "        # Patterns for different types of numbering\n",
    "        self.numbering_patterns = [\n",
    "            # Section numbers like 1.1, 1.2.3, etc.\n",
    "            r'^\\s*\\d+(?:\\.\\d+)*\\s+',\n",
    "            # Simple line numbers at start\n",
    "            r'^\\s*\\d+\\s+',\n",
    "            # Parenthetical numbers like (1), (2), etc.\n",
    "            r'^\\s*\\(\\d+\\)\\s*',\n",
    "            # Letter numbering like a), b), etc.\n",
    "            r'^\\s*[a-zA-Z]\\)\\s*',\n",
    "            # Roman numerals\n",
    "            r'^\\s*[IVXLCDM]+\\.\\s*',\n",
    "            r'^\\s*[ivxlcdm]+\\.\\s*',\n",
    "            # Bullet points with numbers\n",
    "            r'^\\s*[-•*]\\s*\\d+\\.\\s*',\n",
    "            # Sub-section numbers like 5.14.2\n",
    "            r'^Sub-Section\\s+\\d+(?:\\.\\d+)*\\s*',\n",
    "        ]\n",
    "        \n",
    "        # Document identifiers and headers\n",
    "        self.document_patterns = [\n",
    "            r'IW-CDS-\\d+-\\d+\\s*\\([^)]+\\)',  \n",
    "            r'^\\s*\\d+\\s*$',  \n",
    "        ]\n",
    "        \n",
    "    def remove_numbering(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove various types of numbering from the text.\n",
    "        \"\"\"\n",
    "        # lines = text.split('\\n')\n",
    "        lines = text\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            # Skip empty lines\n",
    "            if not line.strip():\n",
    "                cleaned_lines.append('')\n",
    "                continue\n",
    "                \n",
    "            # Remove document identifiers if they're standalone\n",
    "            skip_line = False\n",
    "            for pattern in self.document_patterns:\n",
    "                if re.match(pattern + r'\\s*$', line):\n",
    "                    skip_line = True\n",
    "                    break\n",
    "            \n",
    "            if skip_line:\n",
    "                continue\n",
    "            \n",
    "            # Remove numbering patterns from the beginning of lines\n",
    "            cleaned_line = line\n",
    "            for pattern in self.numbering_patterns:\n",
    "                cleaned_line = re.sub(pattern, '', cleaned_line)\n",
    "            \n",
    "            # Remove inline section references like \"Section 5.14\"\n",
    "            cleaned_line = re.sub(r'\\bSection\\s+\\d+(?:\\.\\d+)*\\b', 'Section', cleaned_line)\n",
    "            \n",
    "            # Remove inline sub-section references\n",
    "            cleaned_line = re.sub(r'\\bSub-Section\\s+\\d+(?:\\.\\d+)*\\b', 'Sub-Section', cleaned_line)\n",
    "            \n",
    "            cleaned_lines.append(cleaned_line)\n",
    "        \n",
    "        return '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    def remove_page_headers_footers(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove common page headers and footers.\n",
    "        \"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        # lines = text\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            # Skip document ID lines that appear as headers/footers\n",
    "            if re.search(r'IW-CDS-\\d+-\\d+.*Revision.*\\d{4}', line):\n",
    "                continue\n",
    "            \n",
    "            # Skip standalone page numbers\n",
    "            if re.match(r'^\\s*\\d{1,4}\\s*$', line):\n",
    "                # Check if it's likely a page number (isolated number line)\n",
    "                if i > 0 and i < len(lines) - 1:\n",
    "                    prev_empty = not lines[i-1].strip()\n",
    "                    next_empty = not lines[i+1].strip()\n",
    "                    if prev_empty or next_empty:\n",
    "                        continue\n",
    "            \n",
    "            cleaned_lines.append(line)\n",
    "        \n",
    "        return '\\n'.join(cleaned_lines)\n",
    "    \n",
    "    def normalize_whitespace(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize whitespace in the text.\n",
    "        \"\"\"\n",
    "        # Replace multiple spaces with single space\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        \n",
    "        # Replace multiple newlines with maximum of two\n",
    "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "        \n",
    "        # Remove trailing whitespace from lines\n",
    "        lines = text.split('\\n')\n",
    "        # lines = text\n",
    "        lines = [line.rstrip() for line in lines]\n",
    "        \n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    def fix_word_breaks(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Fix words that are broken across lines with hyphens.\n",
    "        \"\"\"\n",
    "        # Fix words broken with hyphens at line ends\n",
    "        text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def clean_special_characters(self, text: str, preserve_punctuation: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Clean special characters while preserving readability.\n",
    "        \"\"\"\n",
    "        # Replace common special characters\n",
    "        replacements = {\n",
    "            '\"': '\"',\n",
    "            '\"': '\"',\n",
    "            ''': \"'\",\n",
    "            ''': \"'\",\n",
    "            '–': '-',\n",
    "            '—': '-',\n",
    "            '…': '...',\n",
    "            '\\u00a0': ' ',  # Non-breaking space\n",
    "            '\\u200b': '',   # Zero-width space\n",
    "        }\n",
    "        \n",
    "        for old, new in replacements.items():\n",
    "            text = text.replace(old, new)\n",
    "        \n",
    "        # Remove control characters but preserve newlines and tabs\n",
    "        text = ''.join(char for char in text if char in '\\n\\t' or not ord(char) < 32)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def merge_broken_sentences(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Merge sentences that are incorrectly broken across lines.\n",
    "        \"\"\"\n",
    "        lines = text.split('\\n')\n",
    "        merged_lines = []\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            current_line = lines[i].strip()\n",
    "            \n",
    "            if not current_line:\n",
    "                merged_lines.append('')\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # Check if current line doesn't end with sentence-ending punctuation\n",
    "            # and the next line doesn't start with capital letter (likely continuation)\n",
    "            if (i < len(lines) - 1 and \n",
    "                current_line and \n",
    "                not current_line[-1] in '.!?:;' and\n",
    "                lines[i + 1].strip() and \n",
    "                not re.match(r'^[A-Z\\d\\(\\[]', lines[i + 1].strip())):\n",
    "                \n",
    "                # Merge with next line\n",
    "                next_line = lines[i + 1].strip()\n",
    "                merged_lines.append(current_line + ' ' + next_line)\n",
    "                i += 2\n",
    "            else:\n",
    "                merged_lines.append(current_line)\n",
    "                i += 1\n",
    "        \n",
    "        return '\\n'.join(merged_lines)\n",
    "    \n",
    "    def extract_sections(self, text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Extract sections from the document based on headers.\n",
    "        \"\"\"\n",
    "        sections = {}\n",
    "        current_section = \"Introduction\"\n",
    "        current_content = []\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        for line in lines:\n",
    "            # Check if line is a section header (usually in title case or all caps)\n",
    "            if (len(line.strip()) > 0 and \n",
    "                len(line.strip()) < 100 and\n",
    "                (line.isupper() or \n",
    "                 re.match(r'^[A-Z][A-Za-z\\s]+$', line.strip()) or\n",
    "                 'Section' in line)):\n",
    "                \n",
    "                # Save previous section\n",
    "                if current_content:\n",
    "                    sections[current_section] = '\\n'.join(current_content).strip()\n",
    "                \n",
    "                current_section = line.strip()\n",
    "                current_content = []\n",
    "            else:\n",
    "                current_content.append(line)\n",
    "        \n",
    "        # Save last section\n",
    "        if current_content:\n",
    "            sections[current_section] = '\\n'.join(current_content).strip()\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def preprocess(self, \n",
    "                  text: str,\n",
    "                  remove_numbers: bool = True,\n",
    "                  remove_headers: bool = True,\n",
    "                  normalize_spaces: bool = True,\n",
    "                  fix_breaks: bool = True,\n",
    "                  clean_chars: bool = True,\n",
    "                  merge_sentences: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Apply all preprocessing steps to the text.\n",
    "            text: Input text\n",
    "            remove_numbers: Remove numbering\n",
    "            remove_headers: Remove page headers/footers\n",
    "            normalize_spaces: Normalize whitespace\n",
    "            fix_breaks: Fix word breaks\n",
    "            clean_chars: Clean special characters\n",
    "            merge_sentences: Merge broken sentences\n",
    "        \"\"\"\n",
    "        if remove_numbers:\n",
    "            text = self.remove_numbering(text)\n",
    "        \n",
    "        if remove_headers:\n",
    "            text = self.remove_page_headers_footers(text)\n",
    "        \n",
    "        if fix_breaks:\n",
    "            text = self.fix_word_breaks(text)\n",
    "        \n",
    "        if clean_chars:\n",
    "            text = self.clean_special_characters(text)\n",
    "        if merge_sentences:\n",
    "            text = self.merge_broken_sentences(text)\n",
    "        if normalize_spaces:\n",
    "            text = self.normalize_whitespace(text)\n",
    "        return text\n",
    "    \n",
    "    def process_file(self, \n",
    "                    text: str,\n",
    "                    output_path: Optional[str] = None,\n",
    "                    **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Process text file.\n",
    "        \"\"\"\n",
    "        # Read input file\n",
    "        # with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        #     text = f.read()\n",
    "        output_path = os.getcwd() + \"/output/processed_text.txt\"\n",
    "        # Process text\n",
    "        processed_text = self.preprocess(text, **kwargs)\n",
    "        df= pd.DataFrame([processed_text], columns=['sentences'])\n",
    "        # Save if output path provided\n",
    "        if output_path:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(df)\n",
    "            print(f\"Processed text saved to: {output_path}\")\n",
    "        \n",
    "        return processed_text\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load base model and tokenizer\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading SQuAD dataset...\")\n",
    "dataset = load_dataset('squad')\n",
    "print(\"Dataset structure:\")\n",
    "print(dataset)\n",
    "print(\"\\nFirst training example:\")\n",
    "print(dataset['train'][0])\n",
    "\n",
    "# Process training data with correct structure\n",
    "print(\"\\nProcessing training data...\")\n",
    "train_data = []\n",
    "\n",
    "for item in dataset['train']:\n",
    "    context = item['context']\n",
    "    question = item['question']  # Single question, not a list\n",
    "    answers = item['answers']     # Dictionary with 'text' and 'answer_start'\n",
    "    \n",
    "    # Get the first answer (SQuAD has multiple possible answers)\n",
    "    answer_text = answers['text'][0] if answers['text'] else \"\"\n",
    "    answer_start = answers['answer_start'][0] if answers['answer_start'] else 0\n",
    "    \n",
    "    train_data.append({\n",
    "        'context': context,\n",
    "        'question': question,\n",
    "        'answer': answer_text,\n",
    "        'answer_start': answer_start\n",
    "    })\n",
    "\n",
    "# Process validation data\n",
    "print(\"Processing validation data...\")\n",
    "val_data = []\n",
    "\n",
    "for item in dataset['validation']:\n",
    "    context = item['context']\n",
    "    question = item['question']\n",
    "    answers = item['answers']\n",
    "    \n",
    "    answer_text = answers['text'][0] if answers['text'] else \"\"\n",
    "    answer_start = answers['answer_start'][0] if answers['answer_start'] else 0\n",
    "    \n",
    "    val_data.append({\n",
    "        'context': context,\n",
    "        'question': question,\n",
    "        'answer': answer_text,\n",
    "        'answer_start': answer_start\n",
    "    })\n",
    "\n",
    "# Convert to DataFrames\n",
    "train_df = pd.DataFrame(train_data)\n",
    "val_df = pd.DataFrame(val_data)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "os.makedirs('model/t5/dataset', exist_ok=True)\n",
    "train_df.to_csv(os.getcwd() + '/model/t5/dataset/squad_t5_train.csv', index=False)\n",
    "val_df.to_csv(os.getcwd() + '/model/t5/dataset/squad_t5_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load stored trained squad dataset\n",
    "train_save_path = os.getcwd() + '/model/t5/dataset/squad_t5_train.csv'\n",
    "validation_save_path = os.getcwd() + '/model/t5/dataset/squad_t5_val.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load default dataset and splits for pretraining- already saved in folder\n",
    "train_data = load_dataset('squad', split='train')\n",
    "validate_data = load_dataset('squad', split='validation')\n",
    "sample_validation_dataset = next(iter(validate_data))\n",
    "df_train = pd.DataFrame(train_data, columns = ['context','answers', 'question'])\n",
    "\n",
    "counter_train= 0\n",
    "for index,val in enumerate(tqdm(train_data)):\n",
    "    passage = val['context']\n",
    "    question = val['question']\n",
    "    answer = val['answers']['text'][0]\n",
    "    no_of_words = len(answer.split())\n",
    "    if no_of_words > 5:\n",
    "        df_train.loc[counter_train]= [passage] + [answer] + [question]\n",
    "        counter_train = counter_train + 1\n",
    "\n",
    "counter_val = 0\n",
    "df_validation = pd.DataFrame(validate_data,columns = ['context', 'answers', 'question'])\n",
    "for index,val in enumerate(tqdm(validate_data)):\n",
    "    passage = val['context']\n",
    "    question = val['question']\n",
    "    answer = val['answers']['text'][0]\n",
    "    no_of_words = len(answer.split())\n",
    "    if no_of_words > 5:\n",
    "        df_validation.loc[counter_val]= [passage] + [answer] + [question]\n",
    "        counter_val = counter_val + 1\n",
    "\n",
    "# save the dataset\n",
    "df_train.to_csv(train_save_path, index = False)\n",
    "df_validation.to_csv(validation_save_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question generation class\n",
    "\n",
    "class QuestionGenerationDataset(Dataset):\n",
    "    def __init__(self, tokenizer, filepath, max_len_inp=256,max_len_out=64):\n",
    "        self.path = filepath\n",
    "\n",
    "        self.passage_column = \"context\"\n",
    "        #self.answer = \"answer\"\n",
    "        self.question = \"question\"\n",
    "\n",
    "        self.data = pd.read_csv(self.path)\n",
    "        # self.data = pd.read_csv(self.path,nrows=1000)\n",
    "\n",
    "        self.max_len_input = max_len_inp\n",
    "        self.max_len_output = max_len_out\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self.skippedcount =0\n",
    "        self._build()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  \n",
    "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  \n",
    "\n",
    "        labels = copy.deepcopy(target_ids)\n",
    "        labels [labels==0] = -100\n",
    "\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask,\"labels\":labels}\n",
    "\n",
    "    def _build(self):\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            passage = self.data.loc[idx, self.passage_column]\n",
    "            question = self.data.loc[idx, self.question]\n",
    "            input_ = f\"context: {str(passage)}\" \n",
    "            target = question\n",
    "\n",
    "            # remove encoding length of input to ensure token length check \n",
    "            test_input_encoding = self.tokenizer(input_,\n",
    "                                        truncation=False,\n",
    "                                        return_tensors=\"pt\")\n",
    "\n",
    "            length_of_input_encoding = len(test_input_encoding['input_ids'][0])\n",
    "\n",
    "\n",
    "            if length_of_input_encoding > self.max_len_input:\n",
    "              self.skippedcount = self.skippedcount + 1\n",
    "              continue\n",
    "\n",
    "            # tokenize inputs \n",
    "            tokenized_inputs = self.tokenizer(\n",
    "                input_, max_length=self.max_len_input, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            # tokenize targets\n",
    "            with self.tokenizer.as_target_tokenizer():\n",
    "                tokenized_targets = self.tokenizer(\n",
    "                    target, max_length=self.max_len_output, padding='max_length', truncation=True,return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save validation and train dataset\n",
    "train_dataset = QuestionGenerationDataset(t5_tokenizer,train_save_path)\n",
    "validation_dataset = QuestionGenerationDataset(t5_tokenizer,validation_save_path)\n",
    "# train sample\n",
    "# train_sample = train_dataset[51]\n",
    "# decoded_train_input = t5_tokenizer.decode(train_sample['source_ids'])\n",
    "# decoded_train_output = t5_tokenizer.decode(train_sample['target_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class T5FineTuner(pl.LightningModule):\n",
    "#     def __init__(self,hparams, t5model, t5tokenizer):\n",
    "#         super(T5FineTuner, self).__init__()\n",
    "#         self.hparams = hparams\n",
    "#         self.model = t5model\n",
    "#         self.tokenizer = t5tokenizer\n",
    "\n",
    "\n",
    "#     def forward( self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None):\n",
    "#          outputs = self.model(\n",
    "#             input_ids=input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             decoder_attention_mask=decoder_attention_mask,\n",
    "#             labels=lm_labels,\n",
    "#         )\n",
    "\n",
    "#          return outputs\n",
    "\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         outputs = self.forward(\n",
    "#             input_ids=batch[\"source_ids\"],\n",
    "#             attention_mask=batch[\"source_mask\"],\n",
    "#             decoder_input_ids = batch[\"target_ids\"],\n",
    "#             decoder_attention_mask=batch['target_mask'],\n",
    "#             lm_labels=batch['labels']\n",
    "#         )\n",
    "\n",
    "#         loss = outputs[0]\n",
    "#         self.log('train_loss',loss)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         outputs = self.forward(\n",
    "#             input_ids=batch[\"source_ids\"],\n",
    "#             attention_mask=batch[\"source_mask\"],\n",
    "#             decoder_input_ids = batch[\"target_ids\"],\n",
    "#             decoder_attention_mask=batch['target_mask'],\n",
    "#             lm_labels=batch['labels']\n",
    "#         )\n",
    "\n",
    "#         loss = outputs[0]\n",
    "#         self.log(\"val_loss\",loss)\n",
    "#         return loss\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(train_dataset, \n",
    "#                           batch_size=self.hparams.batch_size,\n",
    "#                           shuffle=True,\n",
    "#                           num_workers=0)\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(validation_dataset, \n",
    "#                           batch_size=self.hparams.batch_size,\n",
    "#                           shuffle=True,\n",
    "#                           num_workers=0)\n",
    "\n",
    "\n",
    "#     # optimizer changed for CPU training to torch.optim.SGD\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.SGD(self.parameters(), lr=3e-4, eps=1e-8)\n",
    "#         return optimizer\n",
    "\n",
    "\n",
    "# args_dict = dict(\n",
    "#     batch_size=1,\n",
    "# )\n",
    "\n",
    "# args = argparse.Namespace(**args_dict)\n",
    "\n",
    "# model = T5FineTuner(args,t5_model,t5_tokenizer)\n",
    "\n",
    "# trainer = pl.Trainer(max_epochs = 4, gpus=0,progress_bar_refresh_rate=30)\n",
    "\n",
    "# trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Configuration for CPU\n",
    "def peft_model(model_name='t5-base'):\n",
    "    \"\"\"Create T5 with LoRA for CPU training\"\"\"\n",
    "    \n",
    "    print(\"Loading base model...\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32,  # FP32 for CPU\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    # LoRA configuration - optimized for CPU\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,  # Rank - higher = better quality, more memory\n",
    "        lora_alpha=32,  # Scaling parameter\n",
    "        target_modules=[\"q\", \"v\"],  # Which layers to adapt\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "    # Output: trainable params: 2,359,296 || all params: 223,020,032 || trainable%: 1.06\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CPUTrainer:\n",
    "    def __init__(self, model, tokenizer, hparams, train_dataset, validation_dataset):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hparams = hparams\n",
    "        self.train_data = train_dataset\n",
    "        self.val_data = validation_dataset\n",
    "        self.device = torch.device('cpu')\n",
    "\n",
    "    def forward( self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None):\n",
    "         outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "         return outputs\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            decoder_input_ids = batch[\"target_ids\"],\n",
    "            decoder_attention_mask=batch['target_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            decoder_input_ids = batch[\"target_ids\"],\n",
    "            decoder_attention_mask=batch['target_mask'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        return loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, \n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, \n",
    "                          batch_size=self.hparams.batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=0)\n",
    "\n",
    "\n",
    "    # optimizer changed for CPU training to torch.optim.SGD\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=3e-4)\n",
    "        return optimizer\n",
    "    \n",
    "    def train(self, epochs=3, batch_size=1, learning_rate=1e-3):\n",
    "        \"\"\"Custom training loop optimized for CPU\"\"\"\n",
    "        \n",
    "        # DataLoader with minimal memory footprint\n",
    "        train_loader = self.train_dataloader()\n",
    "        \n",
    "        validate_loader = self.val_dataloader()\n",
    "            \n",
    "        optimizer = self.configure_optimizers()\n",
    "        \n",
    "            \n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            self.model.train()\n",
    "            train_progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "           \n",
    "            for batch_idx, batch in enumerate(train_progress_bar):    \n",
    "                # Forward pass\n",
    "                train_outputs = self.training_step(batch, batch_idx)\n",
    "                total_loss += train_outputs.item()\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                train_outputs.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update progress bar\n",
    "                train_progress_bar.set_postfix({'loss': train_outputs.item()})\n",
    "                \n",
    "                # Aggressive memory cleanup every 10 steps\n",
    "                if batch_idx % 10 == 0:\n",
    "                    gc.collect()\n",
    "            val_loss= 0\n",
    "            self.model.eval()\n",
    "            validate_progress_bar = tqdm(validate_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, batch in enumerate(validate_progress_bar):  \n",
    "                    # Forward pass\n",
    "                    val_outputs = self.validation_step(batch, batch_idx)\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    validate_progress_bar.set_postfix({'val_loss': val_outputs.item()})\n",
    "                    \n",
    "                    # Aggressive memory cleanup every 10 steps\n",
    "                    if batch_idx % 10 == 0:\n",
    "                        gc.collect()\n",
    "                    \n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(validate_loader)\n",
    "            print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"Epoch {epoch+1} -  Validation Loss: {avg_val_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            self.save_checkpoint(epoch)\n",
    "\n",
    "    def save_checkpoint(self, epoch):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        self.model.save_pretrained(f'./model/lora_checkpoint_epoch_{epoch}')\n",
    "        print(f\"Checkpoint saved for epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "hparams= Namespace(\n",
    "    epochs=3,\n",
    "    batch_size=1,  # Keep at 1 for 8GB RAM\n",
    "    learning_rate=1e-3  # Higher LR works well with LoRA\n",
    ")\n",
    "train_dataset = QuestionGenerationDataset(tokenizer,train_save_path)\n",
    "validation_dataset = QuestionGenerationDataset(tokenizer,validation_save_path)\n",
    "\n",
    "# Create LoRA model\n",
    "model = peft_model('t5-base')\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = CPUTrainer(model, tokenizer, hparams, train_dataset, validation_dataset)\n",
    "\n",
    "# Train - adjust parameters based on your RAM\n",
    "trainer.train(epochs=hparams.epochs)\n",
    "\n",
    "\n",
    "# Save final model should be saved in the same folder\n",
    "model.save_pretrained(os.getcwd() + '/model/final_lora_model')\n",
    "tokenizer.save_pretrained(os.getcwd() + '/model/final_lora_model')\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize includes  boto3 for S3 upload\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "hparams= Namespace(\n",
    "    epochs=3,\n",
    "    batch_size=1,  # Keep at 1 for 8GB RAM\n",
    "    learning_rate=1e-3  # Higher LR works well with LoRA\n",
    ")\n",
    "train_dataset = QuestionGenerationDataset(tokenizer,train_save_path)\n",
    "validation_dataset = QuestionGenerationDataset(tokenizer,validation_save_path)\n",
    "\n",
    "# Create LoRA model\n",
    "model = peft_model('t5-base')\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = CPUTrainer(model, tokenizer, hparams, train_dataset, validation_dataset)\n",
    "\n",
    "# Train - adjust parameters based on your RAM\n",
    "trainer.train(epochs=hparams.epochs)\n",
    "\n",
    "# Save final model in the same location\n",
    "local_path = os.getcwd() + '/model/final_lora_model'\n",
    "model.save_pretrained(local_path)\n",
    "tokenizer.save_pretrained(local_path)\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = \"amzn-agocqs\"\n",
    "for root, dirs, files in os.walk(local_path):\n",
    "        for file in files:\n",
    "            local_file = os.path.join(root, file)\n",
    "            s3_key = f\"models/final/{file}\"\n",
    "            \n",
    "            print(f\"Uploading {file}...\")\n",
    "            s3.upload_file(local_file, bucket_name, s3_key)\n",
    "    \n",
    "print(f\"✓ Model uploaded to s3://{bucket_name}/models/final/\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify AWS account has access to S3 bucket\n",
    "# import sys\n",
    "# import subprocess\n",
    "\n",
    "# def setup_aws_s3():\n",
    "#     \"\"\"Setup S3 access in EC2 container\"\"\"\n",
    "    \n",
    "#     # 1. Install boto3 if needed\n",
    "#     try:\n",
    "#         import boto3\n",
    "#         print(\"✓ boto3 already installed\")\n",
    "#     except ImportError:\n",
    "#         print(\"Installing boto3...\")\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"boto3\"])\n",
    "#         import boto3\n",
    "    \n",
    "#     # 2. Check AWS credentials\n",
    "#     # On EC2 with IAM role, this should work automatically\n",
    "#     try:\n",
    "#         s3 = boto3.client('s3')\n",
    "        \n",
    "#         # Test access\n",
    "#         response = s3.list_buckets()\n",
    "#         print(f\"✓ AWS access working - found {len(response['Buckets'])} buckets\")\n",
    "        \n",
    "#         # Check which credential method is being used\n",
    "#         import boto3.session\n",
    "#         session = boto3.session.Session()\n",
    "#         credentials = session.get_credentials()\n",
    "        \n",
    "#         if credentials:\n",
    "#             # Check if using instance role\n",
    "#             provider = credentials.method\n",
    "#             print(f\"✓ Using credential provider: {provider}\")\n",
    "#             if provider == 'iam-role':\n",
    "#                 print(\"  Using EC2 instance IAM role (best practice!)\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"✗ AWS credentials not configured: {e}\")\n",
    "#         print(\"\\nOptions to fix:\")\n",
    "#         print(\"1. Attach IAM role to EC2 instance (recommended)\")\n",
    "#         print(\"2. Set environment variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY\")\n",
    "#         print(\"3. Mount ~/.aws/credentials in container\")\n",
    "    \n",
    "#     return boto3\n",
    "\n",
    "# # Run setup\n",
    "# boto3 = setup_aws_s3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # Initialize\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    hparams= Namespace(\n",
    "        epochs=3,\n",
    "        batch_size=1,  # Keep at 1 for 8GB RAM\n",
    "        learning_rate=1e-3  # Higher LR works well with LoRA\n",
    "    )\n",
    "    train_dataset = QuestionGenerationDataset(tokenizer,train_save_path)\n",
    "    validation_dataset = QuestionGenerationDataset(tokenizer,validation_save_path)\n",
    "\n",
    "    # Create LoRA model\n",
    "    model = peft_model('t5-base')\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = CPUTrainer(model, tokenizer, hparams, train_dataset, validation_dataset)\n",
    "    \n",
    "    # Train - adjust parameters based on your RAM\n",
    "    trainer.train(epochs=hparams.epochs)\n",
    "\n",
    "    # Save final model should be saved in the same folder\n",
    "    model.save_pretrained(os.getcwd() + '/model/final_lora_model')\n",
    "    tokenizer.save_pretrained(os.getcwd() + '/model/final_lora_model')\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"Saving model\")\n",
    "# save_path_model = os.getcwd() + '/model/t5/model/'\n",
    "# save_path_tokenizer = os.getcwd() + '/model/t5/tokenizer/'\n",
    "# model.model.save_pretrained(save_path_model)\n",
    "# t5_tokenizer.save_pretrained(save_path_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model for inference\n",
    "trained_model_path = os.getcwd() + '/model/final_lora_model' \n",
    "trained_tokenizer = os.getcwd() + '/model/final_lora_tokenizer' \n",
    "tokenizer = T5Tokenizer.from_pretrained(trained_tokenizer)\n",
    "\n",
    "base_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "model = PeftModel.from_pretrained(base_model, trained_model_path)\n",
    "#check device cuda availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"device\", device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions with our data using trained model\n",
    "def generate(df2):\n",
    "    \n",
    "    context = df2['sentences_new']\n",
    "    text = \"context: \"+context\n",
    "    input_ids =[]\n",
    "    attention_mask = []\n",
    "    for x in (text):\n",
    "      encoding = tokenizer.encode_plus(x,max_length =512, padding=True, return_tensors=\"pt\")\n",
    "      print (encoding.keys())\n",
    "      input_ids.append(encoding[\"input_ids\"].to(device))\n",
    "      attention_mask.append(encoding[\"attention_mask\"].to(device))\n",
    "      input_ids, attention_mask = encoding[\"input_ids\"].to(device),encoding[\"attention_mask\"].to(device)\n",
    "    data = dict(zip(input_ids, attention_mask))\n",
    "\n",
    "    model.eval()\n",
    "    cqs = []\n",
    "\n",
    "    for x in (text):\n",
    "      encoding = tokenizer.encode_plus(x,max_length =512, padding=True, return_tensors=\"pt\")\n",
    "      print (encoding.keys())\n",
    "      input_ids, attention_mask = encoding[\"input_ids\"].to(device),encoding[\"attention_mask\"].to(device)\n",
    "      beam_outputs = model.generate(\n",
    "          input_ids=input_ids,attention_mask=attention_mask,\n",
    "          max_length=72,\n",
    "          early_stopping=True,\n",
    "          num_beams=5,\n",
    "          num_return_sequences=3\n",
    "      )\n",
    "\n",
    "      for beam_output in beam_outputs:\n",
    "          sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "          cqs.append(sent)\n",
    "    return cqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing of generated questions\n",
    "def postprocess(cqs):\n",
    "    questions =[]\n",
    "    with open(os.getcwd() + \"/output/CQs.txt\", \"w\") as file:\n",
    "        file.write(str(cqs))\n",
    "    with open(os.getcwd() + \"/output/CQs.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            questions.append(line)\n",
    "    new_questions = []\n",
    "    for i in (questions):\n",
    "        x = str(i).split(\":\")\n",
    "        x = str(i).replace(\"question\", '')\n",
    "        new_questions.append(x)\n",
    "    new_questions2 = str(new_questions).split(\",\")\n",
    "    dcqs =pd.DataFrame(new_questions2)\n",
    "    dcqs.columns = ['CQs']\n",
    "    dcqs['CQs'] = dcqs['CQs'].map(lambda x: x.replace(\":\", '').replace(\"'\", '').replace('[', '').replace(']', '').replace('\"', ''))\n",
    "    \n",
    "    for i in range(0, len(dcqs)):\n",
    "        dcqs.at[i, 'newCQs'] =re.sub(r\"\\s\\([A-Z][a-z]+,\\s[A-Z][a-z]?\\.[^\\)]*,\\s\\d{4}\\)\",\"\",str(dcqs.loc[i,'CQs']))\n",
    "    with open(os.getcwd() + \"/output/cq_output.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(dcqs[\"newCQs\"]))\n",
    "    return dcqs\n",
    "    # # dcqs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abstract pattern extraction\n",
    "def mark_chunk(cq, spans, chunktype, offset, counter):\n",
    "    for (start, end) in spans:  # for each span of EC/PC candidate\n",
    "        cq = cq[:start - offset] + chunktype + str(counter) +\\\n",
    "            cq[end - offset:]  # substitute that candidate with EC/PC marker\n",
    "        offset += (end - start) - len(chunktype) - 1  # check by how much the total length of CQ changed\n",
    "    return cq, offset\n",
    "\n",
    "\n",
    "def extract_EC_chunks(cq):\n",
    "    \"\"\"\n",
    "        Find EC chunks and replace their occurences with EC tags\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_EC_span_reject_wh_starters(chunk):\n",
    "        \"\"\"\n",
    "            By default, SpaCy treats question words (wh- pronouns starting questions: where, what,...)\n",
    "            as nouns, so often if questions starts with wh- pronoun + noun (like: What software is the best?)\n",
    "            the whole \"what software\" is interpreted as EC chunk - this function tries to fix that issue by\n",
    "            omitting wh- word if EC candidate consists of multiple tokens, and first token in question is wh- word.\n",
    "            The same issue occurs for \"How\" starter.\n",
    "            Moreover, chunks extracted with SpaCy enclose words like 'any', 'some' which are important for us, so\n",
    "            they shouldn't be substituted with 'EC' marker. Thus we remove such words if they prepend the EC.\n",
    "            The result is returned as the span - the position of beginning of the fixed EC and position of end.\n",
    "        \"\"\"\n",
    "        if (len(chunk) > 1 and\n",
    "                (chunk[0].text.lower().startswith(\"wh\")) or\n",
    "                (chunk[0].text.lower() == 'how')):\n",
    "            chunk = chunk[1:]\n",
    "\n",
    "        if chunk[0].text.lower() in ['any', 'some', 'many', 'well', 'its']:\n",
    "            chunk = chunk[1:]\n",
    "\n",
    "        return (chunk.start_char, chunk.end_char)\n",
    "\n",
    "    doc = nlp(cq)\n",
    "\n",
    "    #  thins classified as ECs which shouldn't be interpreted that way\n",
    "    rejecting_ec = [\"what\", \"which\", \"when\", \"where\", \"who\", \"type\", \"types\",\n",
    "                    \"kinds\", \"kind\", \"category\", \"categories\", \"difference\",\n",
    "                    \"differences\", \"extent\", \"i\", \"we\", \"respect\", \"there\",\n",
    "                    \"not\", \"the main types\", \"the possible types\", \"the types\",\n",
    "                    \"the difference\", \"the differences\", \"the main categories\"]\n",
    "\n",
    "    counter = 1  # counter indicating current chunk id (EC1, EC2, ...)\n",
    "    offset = 0   # if we replace for example \"Weka\" with 'EC1' then the new CQ will be shorter by one char the offset remembers by how much we have shortened the current CQ with EC markers, so the new substitutions can be applied in correct places.\n",
    "\n",
    "\n",
    "    # we decided to treat qualities defined as adjectives in: How + Quality(adjective) + Verb as EC\n",
    "    if (doc[0].text.lower() == 'how' and\n",
    "            doc[1].pos_ == 'ADJ' and\n",
    "            doc[2].pos_ == 'VERB'):\n",
    "\n",
    "        start = doc[1].idx  # mark where quality starts\n",
    "        end = start + len(doc[1])  # mark where quality ends\n",
    "\n",
    "        cq, offset = mark_chunk(cq, [(start, end)], \"EC\", offset, counter)  # substitute quality with EC identifier\n",
    "        counter += 1  # the next EC chunk should have a new, bigger identifier, for example EC2\n",
    "\n",
    "    for chunk in doc.noun_chunks:  # for each EC chunk candidate detected\n",
    "        (start, end) = _get_EC_span_reject_wh_starters(chunk)  # check where chunk begins and ends\n",
    "\n",
    "        ec = cq[start - offset:end - offset]  # extract text of potential EC, apply offsets\n",
    "\n",
    "        if ec.lower() in rejecting_ec:  # if it should be rejected - do nothing\n",
    "            continue\n",
    "\n",
    "        if \"the thing\" in ec and end - start > len(\"the thing\"):\n",
    "            cq = cq[:start - offset] + \"EC\" + str(counter) +\\\n",
    "                \" EC\" + str(counter + 1) + cq[end - offset:]\n",
    "            counter += 2\n",
    "            offset += (end - start) - 7\n",
    "        else:\n",
    "            cq, offset = mark_chunk(cq, [(start, end)], \"EC\", offset, counter)\n",
    "            counter += 1\n",
    "\n",
    "    if (doc[-2].pos_ == 'VERB' and doc[-3].text in ['are', 'is', 'were', 'was'] and doc[-1].text == '?') or (doc[-2].pos_ in ['ADJ', 'ADV'] and doc[-1].text == '?'):\n",
    "        # if CQ ends with are/is/were/was + VERB + ? or the last token is ADJective or ADVerb, treat the\n",
    "        # verb / adverb / adjective as EC\n",
    "        # Which animals are endangered -> endangered is EC\n",
    "        # Which animals are quick -> quick is EC\n",
    "        if doc[-2].text.lower() not in rejecting_ec:\n",
    "            start = doc[-2].idx\n",
    "            end = start + len(doc[-2])\n",
    "\n",
    "            cq, offset = mark_chunk(\n",
    "                cq, [(start, end)], \"EC\", offset, counter)\n",
    "            counter += 1\n",
    "\n",
    "    return cq\n",
    "\n",
    "\n",
    "def get_PCs_as_spans(cq):\n",
    "    def _is_auxilary(token, chunk_token_ids):\n",
    "        \"\"\"\n",
    "            Check if given token is an auxilary verb of detected PC.\n",
    "            The auxilary verb can be in a different place than the main part\n",
    "            of the PC, so pos-tag-sequence based rules don't work here.\n",
    "            For example in \"What system does Weka require?\" - the main part\n",
    "            of PC is the word 'required'. The auxilary verb 'does' is separeted\n",
    "            from the main part by 'Weka' noun. Thus dependency tree is used\n",
    "            to identify auxilaries.\n",
    "        \"\"\"\n",
    "        if (token.head.i in chunk_token_ids and  # if dep-tree current token's parent (head) is somewhere inside the main part of PC represented as chunk_token_ids (sequence of numeric token identifiers)\n",
    "                token.dep_ == 'aux' and  # if the dep-tree label on the edge between some word from main part of PC and current token is AUX (auxilary)\n",
    "                token.i not in chunk_token_ids):  # if token is outside of detected main part of PC\n",
    "            return True  # yep, it's auxilary\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _get_span(group, doc):\n",
    "        id_tags = group.split(\",\")\n",
    "        ids = [int(id_tag.split(\"::\")[0]) for id_tag in id_tags]\n",
    "        aux = None\n",
    "        for token in doc:\n",
    "            if _is_auxilary(token, ids):\n",
    "                aux = token\n",
    "\n",
    "        return (doc[ids[0]].idx, doc[ids[-1]].idx + len(doc[ids[-1]]),\n",
    "                aux)\n",
    "\n",
    "    def _reject_subspans(spans):\n",
    "        \"\"\"\n",
    "            Given list of (chunk begin index, chunk end index) spans,\n",
    "            return only those spans that aren't subspans of any other span.\n",
    "            For instance form list [(1,10), (2,5)], the second span\n",
    "            will be rejected because it is a subspan of the first one.\n",
    "        \"\"\"\n",
    "        filtered = []\n",
    "        for i, span in enumerate(spans):\n",
    "            subspan = False\n",
    "            for j, other in enumerate(spans):\n",
    "                if i == j:\n",
    "                    continue\n",
    "\n",
    "                if span[0] >= other[0] and span[1] <= other[1]:\n",
    "                    subspan = True\n",
    "                    break\n",
    "            if subspan is False:\n",
    "                filtered.append(span)\n",
    "        return filtered\n",
    "\n",
    "    doc = nlp(cq)\n",
    "\n",
    "    \"\"\"\n",
    "        Transform CQ into a form of POS-tags with token sequence identifier.\n",
    "        Each token is described with \"{ID}::{POS_TAG}\".\n",
    "        Tokens are separated with \",\"\n",
    "        Having that form, we can extract longest sequences of expected pos-tags\n",
    "        using regexes. The extracted parts can be explored to collect identifiers\n",
    "        of tokens, so we know where they are located in text.\n",
    "        Ex: \"Kate owns a cat\" should be translated into: \"1::NOUN,2::VERB,3::DET,4::NOUN\"\n",
    "    \"\"\"\n",
    "    pos_text = \",\".join(\n",
    "        [\"{id}::{pos}\".format(id=id, pos=t.pos_) for id, t in enumerate(doc)])\n",
    "\n",
    "    regexes = [   # rules describing PCs\n",
    "        r\"([0-9]+::(PART|VERB),?)*([0-9]+::VERB)\",\n",
    "        r\"([0-9]+::(PART|VERB),?)+([0-9]+::AD(J|V),)+([0-9]+::ADP)\",\n",
    "        r\"([0-9]+::(PART|VERB),?)+([0-9]+::ADP)\",\n",
    "    ]\n",
    "\n",
    "    spans = []  # list of beginnings and endings of each chunk\n",
    "    for regex in regexes:  # try to extract chunks with regexes\n",
    "        for m in re.finditer(regex, pos_text):\n",
    "            spans.append(_get_span(m.group(), doc))  # get chunk begin and end if matched\n",
    "    spans = _reject_subspans(spans)  # reject subspans\n",
    "    return spans\n",
    "\n",
    "\n",
    "def extract_PC_chunks(cq):\n",
    "    rejecting_pc = ['is', '痴', 'are', 'was', 'do', 'does', 'did', 'were',\n",
    "                    'have', 'had', 'can', 'could', 'categorise', 'regarding',\n",
    "                    'is of', 'are of', 'are in', 'given', 'is there']\n",
    "\n",
    "    offset = 0\n",
    "    counter = 1\n",
    "\n",
    "    for begin, end, aux in get_PCs_as_spans(cq):\n",
    "        if cq[begin - offset:end - offset].lower() in rejecting_pc:\n",
    "            continue\n",
    "        spans = [(begin, end)]\n",
    "        if aux:\n",
    "            spans.insert(0, (aux.idx, aux.idx + len(aux)))\n",
    "\n",
    "        cq, offset = mark_chunk(cq, spans, \"PC\", offset, counter)\n",
    "        counter += 1\n",
    "    return cq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post abstraction processing\n",
    "path_to_output = os.getcwd() + \"/output/output.txt\"\n",
    "def post_abstract(path_to_output):\n",
    "    genQuestion=[] \n",
    "    splitQuestions =[]\n",
    "    patterns = []\n",
    "    pattern_candidates = {}\n",
    "    T5Cqs_patterns = []\n",
    "    with open(path_to_output, 'r') as f:\n",
    "      for line in f.readlines():\n",
    "        line= line.replace('?', '? ')\n",
    "        line2 = line.split(\"?\")\n",
    "        line3= [p.join(' ?') for p in line2]\n",
    "        line3 = [p.lstrip() for p in line3]\n",
    "        genQuestion.append(line3)\n",
    "    genQuestionDF= pd.DataFrame(genQuestion).T\n",
    "    genQuestionDF2 = genQuestionDF.drop_duplicates(subset='questions', keep=\"first\")\n",
    "    genQuestionDF.columns=['questions']\n",
    "\n",
    "    allList=[]\n",
    "    pat_candidates={}\n",
    "    for sublist in  genQuestion: \n",
    "        final = []\n",
    "        for i in sublist:\n",
    "            if(i==''):\n",
    "                continue\n",
    "            for item in i.split(','):\n",
    "                pat = extract_EC_chunks(item)\n",
    "                pat = extract_PC_chunks(pat)\n",
    "                final.append(pat)\n",
    "                pat_candidates[item]=(pat)\n",
    "        allList.append(final)\n",
    "    return allList, pat_candidates, genQuestionDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file for human review where necessary. you import back into the process afterwards here\n",
    "def processCQs():\n",
    "    absWithCQs= pd.DataFrame(pat_candidates.items(), columns=['questions', 'patterns'])\n",
    "    absWithCQs = absWithCQs.iloc[:-1]\n",
    "    absWithCQs.to_csv(os.getcwd() + \"/output/geneCQsNov.csv\", sep='|')\n",
    "    #import CLaRO templates\n",
    "    templates= pd.read_csv(os.getcwd() + \"/CLaROv2.csv\", sep = \";\")\n",
    "    templates.columns = 'ID', 'patterns'\n",
    "    return absWithCQs, templates\n",
    "    #check if present in CLaRO\n",
    "\n",
    "def compare_patterns(absWithCQs, templates):\n",
    "    check = pd.merge(absWithCQs, templates, on=['patterns'], how='left', indicator='Exist')\n",
    "    generated_CQs_with_patterns = check[check['Exist'] == 'both']\n",
    "    check_with_cqPatterns_only = check[check['Exist'] == 'left_only']\n",
    "    check_in__templates_only = check[check['Exist'] == 'right_only']\n",
    "    #extract distict ptterns found in templates \n",
    "    distinct_patterns = generated_CQs_with_patterns.drop_duplicates(subset='patterns', keep=\"first\")\n",
    "    return distinct_patterns, check_in__templates_only, check_with_cqPatterns_only, generated_CQs_with_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic similarity to check CQs that are the same and should be dropped and also find questions that not similar\n",
    "def final_output(genQuestionDF2):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    usedCorpus = list(set(list(genQuestionDF2['questions'])))\n",
    "    query_Corpus =list(set(list(genQuestionDF2['questions'])))\n",
    "    #Compute embedding for both lists\n",
    "    corpus_embed = model.encode(query_Corpus, convert_to_tensor=True)\n",
    "    covidSemanticCQs= {}\n",
    "    top_k = min(5, len(usedCorpus))\n",
    "    for qry in query_Corpus:\n",
    "        query_embed = model.encode(qry, convert_to_tensor=True, device=device)\n",
    "        cos_scores = util.cos_sim(query_embed, corpus_embed)[0]\n",
    "        top_results = torch.topk(cos_scores, k=top_k)\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        new_score= \"{:.4f}\".format(score)\n",
    "        new_score= float(new_score)\n",
    "        if  new_score <= 0.7500:\n",
    "            covidSemanticCQs[qry]=[]\n",
    "            covidSemanticCQs[qry].append({query_Corpus[idx]: (new_score)})\n",
    "        QuesSimilar= pd.json_normalize([ { \"firstCQs\": key_, \"secondCQs\" : i, \"score\" : child[i] }  for key_ in covidSemanticCQs for child in covidSemanticCQs[key_] for i in child ])\n",
    "\n",
    "    QuesSimilar.to_csv(os.getcwd() +  \"/output/semanticCQs.csv\", sep='|')\n",
    "    return QuesSimilar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate CQs      \n",
    "def generateCQs(textFile):   \n",
    "    outText= readTextFile(textFile)\n",
    "    # text = format(outText) I need to add outText to process file (stuck here)\n",
    "    preprocessor= TextPreprocessor()\n",
    "    processed_text = preprocessor.process_file(text=outText, output_path=None,\n",
    "                                               remove_numbers=True,\n",
    "                                               remove_headers=True,\n",
    "                                               normalize_spaces=True,\n",
    "                                               fix_breaks=True,\n",
    "                                               clean_chars=True,\n",
    "                                               merge_sentences=True)\n",
    "    processor = SentenceProcessor()\n",
    "    sentences, corpus_entities = processor.process_text(processed_text)\n",
    "    # get only the sentences for process_test output and generate dataframe\n",
    "    df= pd.DataFrame([sentences])\n",
    "#   rename column\n",
    "    df.columns = ['sentences']\n",
    "    df['sentences_new'] = df['sentences'].map(lambda x: ' '.join(x))\n",
    "    print(\"a look at dataframe\",df.head(10))\n",
    "\n",
    "    cqs = generate(df)\n",
    "    dcqs = postprocess(cqs)\n",
    "    allList, pat_candidates, genQuestionDF2 = post_abstract(path_to_output)\n",
    "    Similar_CQs_filter= final_output(genQuestionDF2)\n",
    "    absWithCQs,templates= processCQs(pat_candidates)\n",
    "    distinct_patterns, check_in_templates_only, check_with_cqPatterns_only, generated_CQs_with_patterns = compare_patterns(absWithCQs, templates)\n",
    "        \n",
    "    return Similar_CQs_filter, distinct_patterns, generated_CQs_with_patterns, corpus_entities\n",
    "    \n",
    "def main(file_paths):\n",
    "    list_text_files = []\n",
    "    text_file_path= get_files(file_paths)\n",
    "    duplicate_CQs = []\n",
    "    duplicate_CQs_df = pd.DataFrame(duplicate_CQs)\n",
    "    files_used =pd.DataFrame(list_text_files)\n",
    "    for textFile in text_file_path:\n",
    "        if textFile.endswith(\".txt\"):\n",
    "            CQs_file_name = Path(textFile).stem\n",
    "            result_name = CQs_file_name.join(\"_CQs\")\n",
    "            Similar_CQs_filter, distinct_patterns, result_name  = generateCQs(textFile)\n",
    "            duplicate_CQs.append(Similar_CQs_filter)\n",
    "            list_text_files.append(CQs_file_name)\n",
    "\n",
    "    # save outputs to file\n",
    "\n",
    "    return list_text_files, Similar_CQs_filter, distinct_patterns, result_name \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    list_text_files, Similar_CQs_filter, distinct_patterns, result_name = main(file_paths)  \n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
