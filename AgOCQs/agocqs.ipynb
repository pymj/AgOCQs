{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#supporting import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import nltk\n",
    "from sklearn.utils import shuffle\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from operator import itemgetter\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "pd.set_option(\"display.max_colwidth\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract sentences\n",
    "\n",
    "def extract_sentences(file):\n",
    "    content = nlp(str(file))\n",
    "    return [x.text for x in content.sents]\n",
    "\n",
    "outText = []\n",
    "text_path = \"/inputText\"\n",
    "files = (file for file in os.listdir(text_path)\n",
    "         if os.path.isfile(os.path.join(text_path, file)))\n",
    "for file in files:\n",
    "  key_name = os.path.splitext(file)[0]\n",
    "  mypdf = open(os.path.join(text_path, file), \"r\")\n",
    "  T = mypdf.readlines()\n",
    "  final = [i.replace('\\r\\n',' ').replace('b\\r\\n', '').replace('\\n', '')  for i in T]\n",
    "  outText.append(T)\n",
    "  # newtext.append(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentText= extract_sentences(outText)\n",
    "final = [i.replace(\"\\n'\",\" \") for i in sentText]\n",
    "#result = re.sub(r'http\\S+', '', str(final))\n",
    "newFinal =[re.sub(\"[\\(%\\[].*?[\\)\\n]]\", \" \", i) for i in final]\n",
    "newFinal =[re.sub(r'http\\S+', '', i) for i in newFinal]\n",
    "df2 =pd.DataFrame(final, columns= ['sentences'])\n",
    "df3 =pd.DataFrame(newFinal, columns= ['sentences'])\n",
    "\n",
    "for i in range(0, len(df2)):\n",
    "    df2.at[i, 'sentences_new'] = re.sub(r'\\b(\\d+\\w{1,2})\\b',\" \",str(df2.loc[i,'sentences']))\n",
    "    df2.at[i, 'sentences_new'] = re.sub(r'[\\d+]',\" \",str(df2.loc[i,'sentences_new']))\n",
    "    df2.at[i, 'sentences_new'] = re.sub(r'[^\\w\\s]',\" \",str(df2.loc[i,'sentences_new']))\n",
    "    #df2.at[i, 'sentences_new'] = re.sub(r'\\b\\w{1}\\b',\" \",str(df2.loc[i,'sentences_new']))\n",
    "df2['text'] = df2['sentences_new'].str.replace(r\"\\s*https?:'//\\S+(\\s+|$)\", \" \").str.strip()\n",
    "df2['text2'] = df2['text'].str.replace(r\"ufeff\", \" \").str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load default dataset and splits for pretraining\n",
    "train_data = load_dataset('squad', split='train')\n",
    "validate_data = load_dataset('squad', split='validation')\n",
    "sample_validation_dataset = next(iter(validate_data))\n",
    "df_train = pd.DataFrame(columns = ['context','answers', 'question'])\n",
    "df_validation = pd.DataFrame(columns = ['context', 'answers', 'question'])\n",
    "counter_train= 0\n",
    "for index,val in enumerate(tqdm(train_data)):\n",
    "    passage = val['context']\n",
    "    question = val['question']\n",
    "    answer = val['answers']['text'][0]\n",
    "    no_of_words = len(answer.split())\n",
    "    if no_of_words > 5:\n",
    "        df_train.loc[counter_train]= [passage] + [answer] + [question]\n",
    "        counter_train = counter_train + 1\n",
    "\n",
    "counter_val = 0\n",
    "df_train = pd.DataFrame(columns=['passage','answer','question'])\n",
    "for index,val in enumerate(tqdm(validate_data)):\n",
    "    passage = val['context']\n",
    "    question = val['question']\n",
    "    answer = val['answers']['text'][0]\n",
    "    no_of_words = len(answer.split())\n",
    "    if no_of_words > 5:\n",
    "        df_validation.loc[counter_val]= [passage] + [answer] + [question]\n",
    "        counter_val = counter_val + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save_path = 'model/t5/dataset/squad_t5_train.csv'\n",
    "validation_save_path = 'model/t5/dataset/squad_t5_train.csv'\n",
    "df_train.to_csv(train_save_path, index = False)\n",
    "df_validation.to_csv(validation_save_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from itertools import chain\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "# from termcolor import colored\n",
    "import textwrap\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load base model and tokenizer\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question generation class\n",
    "import copy\n",
    "class QuestionGenerationDataset(Dataset):\n",
    "    def __init__(self, tokenizer, filepath, max_len_inp=512,max_len_out=96):\n",
    "        self.path = filepath\n",
    "\n",
    "        self.passage_column = \"context\"\n",
    "        #self.answer = \"answer\"\n",
    "        self.question = \"question\"\n",
    "\n",
    "        # self.data = pd.read_csv(self.path)\n",
    "        self.data = pd.read_csv(self.path,nrows=1000)\n",
    "\n",
    "        self.max_len_input = max_len_inp\n",
    "        self.max_len_output = max_len_out\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self.skippedcount =0\n",
    "        self._build()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "        labels = copy.deepcopy(target_ids)\n",
    "        labels [labels==0] = -100\n",
    "\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask,\"labels\":labels}\n",
    "\n",
    "    def _build(self):\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            passage,target = self.data.loc[idx, self.passage_column], self.data.loc[idx, self.question]\n",
    "\n",
    "            input_ = \"context: %s </s>\" % (passage)\n",
    "            target = \"question: %s </s>\" % (str(target))\n",
    "\n",
    "            # get encoding length of input. If it is greater than self.max_len skip it\n",
    "            test_input_encoding = self.tokenizer.encode_plus(input_,\n",
    "                                        truncation=False,\n",
    "                                        return_tensors=\"pt\")\n",
    "\n",
    "            length_of_input_encoding = len(test_input_encoding['input_ids'][0])\n",
    "\n",
    "\n",
    "            if length_of_input_encoding > self.max_len_input:\n",
    "              self.skippedcount = self.skippedcount + 1\n",
    "              continue\n",
    "\n",
    "            # tokenize inputs\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [input_], max_length=self.max_len_input, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            # tokenize targets\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                [target], max_length=self.max_len_output, pad_to_max_length=True,return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QuestionGenerationDataset(t5_tokenizer,train_save_path)\n",
    "validation_dataset = QuestionGenerationDataset(t5_tokenizer,validation_save_path)\n",
    "train_sample = train_dataset[51]\n",
    "decoded_train_input = t5_tokenizer.decode(train_sample['source_ids'])\n",
    "decoded_train_output = t5_tokenizer.decode(train_sample['target_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self,hparams, t5model, t5tokenizer):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.model = t5model\n",
    "        self.tokenizer = t5tokenizer\n",
    "\n",
    "\n",
    "    def forward( self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None):\n",
    "         outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "\n",
    "         return outputs\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            decoder_input_ids = batch[\"target_ids\"],\n",
    "            decoder_attention_mask=batch['target_mask'],\n",
    "            lm_labels=batch['labels']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "        self.log('train_loss',loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.forward(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            decoder_input_ids = batch[\"target_ids\"],\n",
    "            decoder_attention_mask=batch['target_mask'],\n",
    "            lm_labels=batch['labels']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "        self.log(\"val_loss\",loss)\n",
    "        return loss\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, batch_size=self.hparams.batch_size,num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(validation_dataset, batch_size=self.hparams.batch_size,num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=3e-4, eps=1e-8)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "args = argparse.Namespace(**args_dict)\n",
    "\n",
    "model = T5FineTuner(args,t5_model,t5_tokenizer)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs = 1, gpus=1,progress_bar_refresh_rate=30)\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "print (\"Saving model\")\n",
    "save_path_model = 'model/t5/model/'\n",
    "save_path_tokenizer = 'model/t5/tokenizer/'\n",
    "model.model.save_pretrained(save_path_model)\n",
    "t5_tokenizer.save_pretrained(save_path_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
    "trained_model_path = '/model/t5/model/'\n",
    "trained_tokenizer = '/model/t5/tokenizer/'\n",
    "model = T5ForConditionalGeneration.from_pretrained(trained_model_path)\n",
    "tokenizer = T5Tokenizer.from_pretrained(trained_tokenizer)\n",
    "#check device cude availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (\"device \",device)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions with our data using trained model\n",
    "context = df2['sentences_new']\n",
    "text = \"context: \"+context\n",
    "input_ids =[]\n",
    "attention_mask = []\n",
    "for x in (text):\n",
    "  encoding = tokenizer.encode_plus(x,max_length =512, padding=True, return_tensors=\"pt\")\n",
    "  print (encoding.keys())\n",
    "  input_ids.append(encoding[\"input_ids\"].to(device))\n",
    "  attention_mask.append(encoding[\"attention_mask\"].to(device))\n",
    "  input_ids, attention_mask = encoding[\"input_ids\"].to(device),encoding[\"attention_mask\"].to(device)\n",
    "data = dict(zip(input_ids, attention_mask))\n",
    "\n",
    "model.eval()\n",
    "cqs = []\n",
    "\n",
    "for x in (text):\n",
    "  encoding = tokenizer.encode_plus(x,max_length =512, padding=True, return_tensors=\"pt\")\n",
    "  print (encoding.keys())\n",
    "  input_ids, attention_mask = encoding[\"input_ids\"].to(device),encoding[\"attention_mask\"].to(device)\n",
    "  beam_outputs = model.generate(\n",
    "      input_ids=input_ids,attention_mask=attention_mask,\n",
    "      max_length=72,\n",
    "      early_stopping=True,\n",
    "      num_beams=5,\n",
    "      num_return_sequences=3\n",
    "  )\n",
    "\n",
    "  for beam_output in beam_outputs:\n",
    "      sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "      cqs.append(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing of generated questions\n",
    "questions =[]\n",
    "with open(\"CQs.txt\", \"w\") as file:\n",
    "    file.write(str(cqs))\n",
    "with open(\"./CQs.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "  for line in f.readlines():\n",
    "    questions.append(line)\n",
    "new_questions = []\n",
    "for i in (questions):\n",
    "  x = str(i).split(\":\")\n",
    "  x = str(i).replace(\"question\", '')\n",
    "  new_questions.append(x)\n",
    "new_questions2 = str(new_questions).split(\",\")\n",
    "dcqs =pd.DataFrame(new_questions2)\n",
    "dcqs.columns = ['CQs']\n",
    "dcqs['CQs'] = dcqs['CQs'].map(lambda x: x.replace(\":\", '').replace(\"'\", '').replace('[', '').replace(']', '').replace('\"', ''))\n",
    "# dcqs.head()\n",
    "new_questions = []\n",
    "for i in (questions):\n",
    "  x = str(i).split(\":\")\n",
    "  x = str(i).replace(\"question\", '')\n",
    "  new_questions.append(x)\n",
    "new_questions2 = str(new_questions).split(\",\")\n",
    "dcqs =pd.DataFrame(new_questions2)\n",
    "dcqs.columns = ['CQs']\n",
    "dcqs['CQs'] = dcqs['CQs'].map(lambda x: x.replace(\":\", '').replace(\"'\", '').replace('[', '').replace(']', '').replace('\"', ''))\n",
    "dcqs.head()\n",
    "#new_questions\n",
    "\n",
    "for i in range(0, len(dcqs)):\n",
    "    dcqs.at[i, 'newCQs'] =re.sub(r\"\\s\\([A-Z][a-z]+,\\s[A-Z][a-z]?\\.[^\\)]*,\\s\\d{4}\\)\",\"\",str(dcqs.loc[i,'CQs']))\n",
    "with open(\"output.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(dcqs[\"newCQs\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abstract pattern extraction\n",
    "def mark_chunk(cq, spans, chunktype, offset, counter):\n",
    "    for (start, end) in spans:  # for each span of EC/PC candidate\n",
    "        cq = cq[:start - offset] + chunktype + str(counter) +\\\n",
    "            cq[end - offset:]  # substitute that candidate with EC/PC marker\n",
    "        offset += (end - start) - len(chunktype) - 1  # check by how much the total length of CQ changed\n",
    "    return cq, offset\n",
    "\n",
    "\n",
    "def extract_EC_chunks(cq):\n",
    "    \"\"\"\n",
    "        Find EC chunks and replace their occurences with EC tags\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_EC_span_reject_wh_starters(chunk):\n",
    "        \"\"\"\n",
    "            By default, SpaCy treats question words (wh- pronouns starting questions: where, what,...)\n",
    "            as nouns, so often if questions starts with wh- pronoun + noun (like: What software is the best?)\n",
    "            the whole \"what software\" is interpreted as EC chunk - this function tries to fix that issue by\n",
    "            omitting wh- word if EC candidate consists of multiple tokens, and first token in question is wh- word.\n",
    "            The same issue occurs for \"How\" starter.\n",
    "            Moreover, chunks extracted with SpaCy enclose words like 'any', 'some' which are important for us, so\n",
    "            they shouldn't be substituted with 'EC' marker. Thus we remove such words if they prepend the EC.\n",
    "            The result is returned as the span - the position of beginning of the fixed EC and position of end.\n",
    "        \"\"\"\n",
    "        if (len(chunk) > 1 and\n",
    "                (chunk[0].text.lower().startswith(\"wh\")) or\n",
    "                (chunk[0].text.lower() == 'how')):\n",
    "            chunk = chunk[1:]\n",
    "\n",
    "        if chunk[0].text.lower() in ['any', 'some', 'many', 'well', 'its']:\n",
    "            chunk = chunk[1:]\n",
    "\n",
    "        return (chunk.start_char, chunk.end_char)\n",
    "\n",
    "    doc = nlp(cq)\n",
    "\n",
    "    #  thins classified as ECs which shouldn't be interpreted that way\n",
    "    rejecting_ec = [\"what\", \"which\", \"when\", \"where\", \"who\", \"type\", \"types\",\n",
    "                    \"kinds\", \"kind\", \"category\", \"categories\", \"difference\",\n",
    "                    \"differences\", \"extent\", \"i\", \"we\", \"respect\", \"there\",\n",
    "                    \"not\", \"the main types\", \"the possible types\", \"the types\",\n",
    "                    \"the difference\", \"the differences\", \"the main categories\"]\n",
    "\n",
    "    counter = 1  # counter indicating current chunk id (EC1, EC2, ...)\n",
    "    offset = 0   # if we replace for example \"Weka\" with 'EC1' then the new CQ will be shorter by one char the offset remembers by how much we have shortened the current CQ with EC markers, so the new substitutions can be applied in correct places.\n",
    "\n",
    "\n",
    "    # we decided to treat qualities defined as adjectives in: How + Quality(adjective) + Verb as EC\n",
    "    if (doc[0].text.lower() == 'how' and\n",
    "            doc[1].pos_ == 'ADJ' and\n",
    "            doc[2].pos_ == 'VERB'):\n",
    "\n",
    "        start = doc[1].idx  # mark where quality starts\n",
    "        end = start + len(doc[1])  # mark where quality ends\n",
    "\n",
    "        cq, offset = mark_chunk(cq, [(start, end)], \"EC\", offset, counter)  # substitute quality with EC identifier\n",
    "        counter += 1  # the next EC chunk should have a new, bigger identifier, for example EC2\n",
    "\n",
    "    for chunk in doc.noun_chunks:  # for each EC chunk candidate detected\n",
    "        (start, end) = _get_EC_span_reject_wh_starters(chunk)  # check where chunk begins and ends\n",
    "\n",
    "        ec = cq[start - offset:end - offset]  # extract text of potential EC, apply offsets\n",
    "\n",
    "        if ec.lower() in rejecting_ec:  # if it should be rejected - do nothing\n",
    "            continue\n",
    "\n",
    "        if \"the thing\" in ec and end - start > len(\"the thing\"):\n",
    "            cq = cq[:start - offset] + \"EC\" + str(counter) +\\\n",
    "                \" EC\" + str(counter + 1) + cq[end - offset:]\n",
    "            counter += 2\n",
    "            offset += (end - start) - 7\n",
    "        else:\n",
    "            cq, offset = mark_chunk(cq, [(start, end)], \"EC\", offset, counter)\n",
    "            counter += 1\n",
    "\n",
    "    if (doc[-2].pos_ == 'VERB' and doc[-3].text in ['are', 'is', 'were', 'was'] and doc[-1].text == '?') or (doc[-2].pos_ in ['ADJ', 'ADV'] and doc[-1].text == '?'):\n",
    "        # if CQ ends with are/is/were/was + VERB + ? or the last token is ADJective or ADVerb, treat the\n",
    "        # verb / adverb / adjective as EC\n",
    "        # Which animals are endangered -> endangered is EC\n",
    "        # Which animals are quick -> quick is EC\n",
    "        if doc[-2].text.lower() not in rejecting_ec:\n",
    "            start = doc[-2].idx\n",
    "            end = start + len(doc[-2])\n",
    "\n",
    "            cq, offset = mark_chunk(\n",
    "                cq, [(start, end)], \"EC\", offset, counter)\n",
    "            counter += 1\n",
    "\n",
    "    return cq\n",
    "\n",
    "\n",
    "def get_PCs_as_spans(cq):\n",
    "    def _is_auxilary(token, chunk_token_ids):\n",
    "        \"\"\"\n",
    "            Check if given token is an auxilary verb of detected PC.\n",
    "            The auxilary verb can be in a different place than the main part\n",
    "            of the PC, so pos-tag-sequence based rules don't work here.\n",
    "            For example in \"What system does Weka require?\" - the main part\n",
    "            of PC is the word 'required'. The auxilary verb 'does' is separeted\n",
    "            from the main part by 'Weka' noun. Thus dependency tree is used\n",
    "            to identify auxilaries.\n",
    "        \"\"\"\n",
    "        if (token.head.i in chunk_token_ids and  # if dep-tree current token's parent (head) is somewhere inside the main part of PC represented as chunk_token_ids (sequence of numeric token identifiers)\n",
    "                token.dep_ == 'aux' and  # if the dep-tree label on the edge between some word from main part of PC and current token is AUX (auxilary)\n",
    "                token.i not in chunk_token_ids):  # if token is outside of detected main part of PC\n",
    "            return True  # yep, it's auxilary\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _get_span(group, doc):\n",
    "        id_tags = group.split(\",\")\n",
    "        ids = [int(id_tag.split(\"::\")[0]) for id_tag in id_tags]\n",
    "        aux = None\n",
    "        for token in doc:\n",
    "            if _is_auxilary(token, ids):\n",
    "                aux = token\n",
    "\n",
    "        return (doc[ids[0]].idx, doc[ids[-1]].idx + len(doc[ids[-1]]),\n",
    "                aux)\n",
    "\n",
    "    def _reject_subspans(spans):\n",
    "        \"\"\"\n",
    "            Given list of (chunk begin index, chunk end index) spans,\n",
    "            return only those spans that aren't subspans of any other span.\n",
    "            For instance form list [(1,10), (2,5)], the second span\n",
    "            will be rejected because it is a subspan of the first one.\n",
    "        \"\"\"\n",
    "        filtered = []\n",
    "        for i, span in enumerate(spans):\n",
    "            subspan = False\n",
    "            for j, other in enumerate(spans):\n",
    "                if i == j:\n",
    "                    continue\n",
    "\n",
    "                if span[0] >= other[0] and span[1] <= other[1]:\n",
    "                    subspan = True\n",
    "                    break\n",
    "            if subspan is False:\n",
    "                filtered.append(span)\n",
    "        return filtered\n",
    "\n",
    "    doc = nlp(cq)\n",
    "\n",
    "    \"\"\"\n",
    "        Transform CQ into a form of POS-tags with token sequence identifier.\n",
    "        Each token is described with \"{ID}::{POS_TAG}\".\n",
    "        Tokens are separated with \",\"\n",
    "        Having that form, we can extract longest sequences of expected pos-tags\n",
    "        using regexes. The extracted parts can be explored to collect identifiers\n",
    "        of tokens, so we know where they are located in text.\n",
    "        Ex: \"Kate owns a cat\" should be translated into: \"1::NOUN,2::VERB,3::DET,4::NOUN\"\n",
    "    \"\"\"\n",
    "    pos_text = \",\".join(\n",
    "        [\"{id}::{pos}\".format(id=id, pos=t.pos_) for id, t in enumerate(doc)])\n",
    "\n",
    "    regexes = [   # rules describing PCs\n",
    "        r\"([0-9]+::(PART|VERB),?)*([0-9]+::VERB)\",\n",
    "        r\"([0-9]+::(PART|VERB),?)+([0-9]+::AD(J|V),)+([0-9]+::ADP)\",\n",
    "        r\"([0-9]+::(PART|VERB),?)+([0-9]+::ADP)\",\n",
    "    ]\n",
    "\n",
    "    spans = []  # list of beginnings and endings of each chunk\n",
    "    for regex in regexes:  # try to extract chunks with regexes\n",
    "        for m in re.finditer(regex, pos_text):\n",
    "            spans.append(_get_span(m.group(), doc))  # get chunk begin and end if matched\n",
    "    spans = _reject_subspans(spans)  # reject subspans\n",
    "    return spans\n",
    "\n",
    "\n",
    "def extract_PC_chunks(cq):\n",
    "    rejecting_pc = ['is', '痴', 'are', 'was', 'do', 'does', 'did', 'were',\n",
    "                    'have', 'had', 'can', 'could', 'categorise', 'regarding',\n",
    "                    'is of', 'are of', 'are in', 'given', 'is there']\n",
    "\n",
    "    offset = 0\n",
    "    counter = 1\n",
    "\n",
    "    for begin, end, aux in get_PCs_as_spans(cq):\n",
    "        if cq[begin - offset:end - offset].lower() in rejecting_pc:\n",
    "            continue\n",
    "\n",
    "        spans = [(begin, end)]\n",
    "\n",
    "        if aux:\n",
    "            spans.insert(0, (aux.idx, aux.idx + len(aux)))\n",
    "\n",
    "        cq, offset = mark_chunk(cq, spans, \"PC\", offset, counter)\n",
    "        counter += 1\n",
    "\n",
    "    return cq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post abstarction processing\n",
    "genQuestion=[] \n",
    "splitQuestions =[]\n",
    "patterns = []\n",
    "pattern_candidates = {}\n",
    "T5Cqs_patterns = []\n",
    "path_to_output = \"./output.txt\"\n",
    "with open(path_to_output, 'r') as f:\n",
    "  for line in f.readlines():\n",
    "    line= line.replace('?', '? ')\n",
    "    line2 = line.split(\"?\")\n",
    "    line3= [p.join(' ?') for p in line2]\n",
    "    line3 = [p.lstrip() for p in line3]\n",
    "    genQuestion.append(line3)\n",
    "genQuestionDF= pd.DataFrame(genQuestion).T\n",
    "genQuestionDF2 = genQuestionDF.drop_duplicates(subset='questions', keep=\"first\")\n",
    "genQuestionDF.columns=['questions']\n",
    "\n",
    "allList=[]\n",
    "pat_candidates={}\n",
    "for sublist in  genQuestion: \n",
    "  final = []\n",
    "  for i in sublist:\n",
    "    if(i==''):\n",
    "      continue\n",
    "    for item in i.split(','):\n",
    "      pat = extract_EC_chunks(item)\n",
    "      pat = extract_PC_chunks(pat)\n",
    "      final.append(pat)\n",
    "      pat_candidates[item]=(pat)\n",
    "      \n",
    "  allList.append(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absWithCQs= pd.DataFrame(pat_candidates.items(), columns=['questions', 'patterns'])\n",
    "absWithCQs = absWithCQs.iloc[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file for human review where necessary. you import back into the process afterwars here\n",
    "absWithCQs.to_csv(\"/CQs/geneCQsNov.csv\", sep='|')\n",
    "#import CLaRO templates\n",
    "templates= pd.read_csv(\"/CLaROv2.csv\", sep = \";\")\n",
    "templates.columns = 'ID', 'patterns'\n",
    "#check if present in CLaRO\n",
    "\n",
    "def compare_patterns(absWithCQs, templates):\n",
    "    check = pd.merge(absWithCQs, templates, on=['patterns'], how='left', indicator='Exist')\n",
    "    check_in_both = check[check['Exist'] == 'both']\n",
    "    check_in_cqPatterns_only = check[check['Exist'] == 'left_only']\n",
    "    check_in__templates_only = check[check['Exist'] == 'right_only']\n",
    "    #extract distict ptterns found in templates \n",
    "    distinct_patterns = check_in_both.drop_duplicates(subset='patterns', keep=\"first\")\n",
    "    return distinct_patterns, check_in__templates_only, check_in_cqPatterns_only, check_in_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semantic similarity to check CQs that are the same and should be dropped and also find questions that not similar\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "CovidCorpus = list(set(list(genQuestionDF2['questions'])))\n",
    "query_Corpus =list(set(list(genQuestionDF2['questions'])))\n",
    "#Compute embedding for both lists\n",
    "corpus_embed = model.encode(query_Corpus, convert_to_tensor=True)\n",
    "covidSemanticCQs= {}\n",
    "top_k = min(5, len(CovidCorpus))\n",
    "for qry in query_Corpus:\n",
    "    query_embed = model.encode(qry, convert_to_tensor=True, device=device)\n",
    "    cos_scores = util.cos_sim(query_embed, corpus_embed)[0]\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "for score, idx in zip(top_results[0], top_results[1]):\n",
    "    new_score= \"{:.4f}\".format(score)\n",
    "    new_score= float(new_score)\n",
    "    if  new_score <= 0.7500:\n",
    "        covidSemanticCQs[qry]=[]\n",
    "        covidSemanticCQs[qry].append({query_Corpus[idx]: (new_score)})\n",
    "    QuesSimilar= pd.json_normalize([ { \"firstCQs\": key_, \"secondCQs\" : i, \"score\" : child[i] }  for key_ in covidSemanticCQs for child in covidSemanticCQs[key_] for i in child ])\n",
    "    \n",
    "QuesSimilar.to_csv(\"/semanticCQs.csv\", sep='|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
